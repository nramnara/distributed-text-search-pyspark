{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**EECS4415N Assignment 2, Winter 2025**\n",
        "\n",
        "**Full Name:** Nicholas Ramnaraign\n",
        "\n",
        "**Student Number:** 211616406"
      ],
      "metadata": {
        "id": "ufopE8oUS7KL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Please do not use any other libraries other than the ones imported below.**\n",
        "\n",
        "**Please write your code in the \"Your code\" parts.**"
      ],
      "metadata": {
        "id": "uPXnAc5jTGzP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "0GmpzevMPFT6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67195fb2-bac4-4fb5-b4d6-8a213a4e2cb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.4)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "The following additional packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra fonts-nanum fonts-ipafont-gothic\n",
            "  fonts-ipafont-mincho fonts-wqy-microhei fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 3 newly installed, 0 to remove and 21 not upgraded.\n",
            "Need to get 39.7 MB of archives.\n",
            "After this operation, 144 MB of additional disk space will be used.\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 124926 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u442-b06~us1-0ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u442-b06~us1-0ubuntu1~22.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u442-b06~us1-0ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u442-b06~us1-0ubuntu1~22.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u442-b06~us1-0ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u442-b06~us1-0ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive2\n",
        "\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlWciRMuNiUb",
        "outputId": "034f37f3-0971-48e4-fae2-8824f376c245"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Connect to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PTcFt7UMPDiq"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.rdd import RDD\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "import math\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2B2Yg9wFP9ci"
      },
      "outputs": [],
      "source": [
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"VSM Retrieval\").getOrCreate()\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTtRiPOH8KDf"
      },
      "source": [
        "**Read all the text files into a RDD named text_rdd**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "eT9BK6djQB4p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71e7536c-a341-46fa-a312-ed56d86a1714"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of documents:\n",
            "1460\n"
          ]
        }
      ],
      "source": [
        "# Define path to input text files (change this to your directory)\n",
        "input_dir = \"/content/drive/MyDrive/YourPath/CISI-IndividualDocuments/\"\n",
        "\n",
        "# Load all text files into an RDD\n",
        "text_rdd = sc.wholeTextFiles(input_dir)  # Each element is (filename, content)\n",
        "print(\"Total number of documents:\")\n",
        "print(text_rdd.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GY2h8E1Xdpmf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "ibPHKq8w5h9I",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "013072ce-1e3a-4120-df64-edb853446d7c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('file:/content/drive/MyDrive/YourPath/CISI-IndividualDocuments/461.txt',\n",
              "  \"Information Retrieval and Processing\\n    The present book embodies a change in structure and focus to reflect the fact that the reader of today's book is much more likely to be an interested college student with a great awareness of the current information revolution than was the case ten years ago.  Thus, hardware, materials, and processes used in connection with information systems are discussed first, in Chapters Two through Four.  The subject of information retrieval per se begins with Chapters Five and Six, which have to do with librarianship  and documentation.  Because of their somewhat historical slant, these chapters (along with Seven) are the only ones taken from the 1963 book which adhere to their original character.  Chapter Seven presents a simplified concept of an information system and its components, and paves the way for discussion of computerized retrieval in the chapters to follow, especially for data retrieval in Chapter Eight and document retrieval in Chapter Nine. Chapters Ten through Twelve, on language processing, evaluation, and user studies, describe important facets of the information retrieval field that have developed strongly since 1963. \\n\"),\n",
              " ('file:/content/drive/MyDrive/YourPath/CISI-IndividualDocuments/462.txt',\n",
              "  'Information Retrieval\\n  The material of this book is aimed at advanced undergraduate information (or computer) science students, postgraduate library science students, and research workers in the field of IR.  Some of the chapters, particularly Chapter 6, make simple use of a little advanced mathematics.  However, the necessary mathematical tools can be easily mastered from numerous mathematical texts that now exist and in any case references have been given where the mathematics occur.\\n'),\n",
              " ('file:/content/drive/MyDrive/YourPath/CISI-IndividualDocuments/463.txt',\n",
              "  'Information Retrieval; British and American, 1876-1976\\n        Of eight chapters this first one deals with principles and definitions and then with the slow development of information retrieval through about 5,000 years until the introduction of printing in Europe less than 500 years prior to our period of principal coverage, 1876-1976.  This latter period coincides with the second century of the United States of America, during which were intensified earlier efforts to carry out one of Washington\\'s urgings in his Farewell Address:  \"Promote then, as an object of primary importance, institutions for the general diffusion of knowledge.\"\\n'),\n",
              " ('file:/content/drive/MyDrive/YourPath/CISI-IndividualDocuments/464.txt',\n",
              "  'Similarity Relations and Fuzzy Orderings\\n   The notion of \"similarity\" as defined in this paper is essentially a  generalization of the notion of equivalence.. In the same vein, a fuzzy ordering is a generalization of the concept of ordering.. For example, the relation  x >> y is a fuzzy linear ordering in the set of real numbers..    More correctly, a similarity relation, S is a fuzzy relation which is reflexive, symmetric, and transitive..    Various properties of similarity relations and fuzzy ordering are  investigated and, as an illustration, an extended version of Szpilrajn\\'s  theorem is proved..\\n'),\n",
              " ('file:/content/drive/MyDrive/YourPath/CISI-IndividualDocuments/465.txt',\n",
              "  'Using Commercially Available Literature Tapes \\n   The paper reviews the need for current awareness services and describes the  basic characteristics of SDI, indicating its advantages.. Details are given of  the problems that have arisen in providing an SDI service based on Chemical  Titles tapes at Aldermaston with particular reference to program limitations..  Data on operating costs and on use assessments of the service are given.. The  pros and cons of title-only alerting systems are discussed..\\n')]"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "# See the top 5 elements of text_rdd\n",
        "text_rdd.take(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Text Preprocessing"
      ],
      "metadata": {
        "id": "yik-JcN89EJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Read a list of stopwords from a file named stop_words.lst (which is a text file with one stopword per line) from the current working directory**"
      ],
      "metadata": {
        "id": "L8D-jOWU0FJj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "69d9UZ4_vrsm"
      },
      "outputs": [],
      "source": [
        "# Read the stopwords from the stop_words.txt file and put them in a set named stop_words\n",
        "# Your code\n",
        "\n",
        "# Path to the stop_words.lst file\n",
        "path_stop_words = \"/content/drive/MyDrive/YourPath/stop_words.lst\"\n",
        "\n",
        "# Reads the list of stop words from the file\n",
        "stop_words = set(sc.textFile(path_stop_words).collect())\n",
        "# print(list(stop_words)[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JgBT1LCs5i3"
      },
      "source": [
        "**Define a function to get the filename prefix without the full path name and the file extension. The prefix will be used as the document ID.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "g47HsyCnsvAC"
      },
      "outputs": [],
      "source": [
        "def get_filename_prefix(filepath):\n",
        "  \"\"\"\n",
        "  Extracts the filename prefix from a given filepath.\n",
        "  Args:\n",
        "    filepath: The full path to the file.\n",
        "  Returns:\n",
        "    The filename prefix (without extension or path), or None if the input is invalid.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    filename = os.path.basename(filepath)  # Get the filename from the path\n",
        "    filename_without_ext = os.path.splitext(filename)[0]  # Remove the extension\n",
        "    return filename_without_ext\n",
        "  except:\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4E-Gy3To-jT0"
      },
      "source": [
        "**Convert text_rdd into another RDD (doc_words_rdd) whose element is (doc_id, word_list) where doc_id is the file name prefix of a document and word_list is the list of words in the document after preprocessing. Process the content of each document in text_rdd by:**\n",
        "\n",
        "*   keeping only the words consisting of only alphabetic letters (terms like 18, A20, 20K should be excluded)  \n",
        "*   converting the words to lower cases\n",
        "*   removing stopwords\n",
        "*   converting the words into word stems using the Porter Stemmer in NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "0GvagObEQ71q"
      },
      "outputs": [],
      "source": [
        "# Use the Porter Stemmer from NLTK\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(document):\n",
        "  \"\"\"\n",
        "  Convert a document into a list of words after preprocessing.\n",
        "  Args:\n",
        "    document: (the full path to the document file, the content of the file)\n",
        "  Returns:\n",
        "    (doc_id, a list of words after preprocessing))\n",
        "  \"\"\"\n",
        "  filename, content = document\n",
        "  words = re.findall(r\"\\b[a-z]+\\b\", content.lower())  # Tokenization\n",
        "\n",
        "  # Your code for stopword removing and stemming\n",
        "\n",
        "  # Initialize lists.\n",
        "  filtered_words = []\n",
        "  stemmed_words = []\n",
        "\n",
        "  # Loop through words and only keep the ones that aren't stop words.\n",
        "  for word in words:\n",
        "    if word not in stop_words:\n",
        "      filtered_words.append(word)\n",
        "\n",
        "  # Loop through the words and apply stemming.\n",
        "  for word in filtered_words:\n",
        "      # stemmed_word = stemmer.stem(word)\n",
        "      stemmed_words.append(stemmer.stem(word))\n",
        "\n",
        "  # Reassign stemmed_words to filtered_words because that is what we are expected to return.\n",
        "  filtered_words = stemmed_words\n",
        "\n",
        "  # Return filtered and stemmed words.\n",
        "  return (get_filename_prefix(filename), filtered_words)  # (doc_id, [word1, word2, word3, ...])\n",
        "\n",
        "# Convert text_rdd into another RDD (doc_words_rdd) whose elment is (doc_id, word_list)\n",
        "doc_words_rdd = text_rdd.map(preprocess_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Build Inverted Index"
      ],
      "metadata": {
        "id": "iViKZvj4-PgU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_xTV9TyBZ3D"
      },
      "source": [
        "**Compute term frequency (tf_value) of each word in each document, and put the results in a RDD (tf_rdd) where each element is (word, (doc_id, tf_value)). The relative term frequency value should be used, that is, the tf_value of a word $w$ in a document $d$ should be the number of occurrences of $w$ in $d$ divided by the total number of words in $d$.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "ATDC4PRHRHPj"
      },
      "outputs": [],
      "source": [
        "# Compute Term Frequency (TF)\n",
        "def compute_tf(doc_tuple):\n",
        "    \"\"\"\n",
        "    Compute term frequency (TF) for each word in a document.\n",
        "    Args:\n",
        "        doc_tuple: (doc_id, [word1, word2, word3, ...])\n",
        "    Returns:\n",
        "        A list of (word, (doc_id, tf_value)) tuples.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # Your Code\n",
        "\n",
        "    # Unpack the tuple to get words and doc_id.\n",
        "    doc_id, words = doc_tuple\n",
        "\n",
        "    # Caclulates the frequency of words,\n",
        "    frequency_words = {}\n",
        "\n",
        "    # Lopp through words and count the frequency, storing the value in the\n",
        "    # freq_words dictionary.\n",
        "    for word in words:\n",
        "      if word in frequency_words:\n",
        "        frequency_words[word] += 1\n",
        "      else:\n",
        "        frequency_words[word] = 1\n",
        "\n",
        "    # Get the total number of words.\n",
        "    num_words = len(words)\n",
        "\n",
        "    # Initialize term frequency list.\n",
        "    tf_list = []\n",
        "\n",
        "    # Compute term frequency and add to the return list.\n",
        "    # Term frequency = frequency of word / total number of words.\n",
        "    # Stored as (word, (doc_id, tf)).\n",
        "    for word in frequency_words:\n",
        "      tf_list.append((word, (doc_id, frequency_words[word]/num_words)))\n",
        "\n",
        "    return tf_list # York Code\n",
        "\n",
        "# Convert doc_words_rdd into tf_rdd whiose element is (word, (doc_id, tf_value))\n",
        "tf_rdd = doc_words_rdd.flatMap(compute_tf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJmH7bFRMqCO"
      },
      "source": [
        "**From tf_rdd, compute the document frequency of each word and put the results in a RDD (df_rdd), where each element is (word, doc_count)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "YcM4wfCxbmoT"
      },
      "outputs": [],
      "source": [
        "# Compute Document Frequency (DF) per word\n",
        "\n",
        "# Your Code\n",
        "\n",
        "\n",
        "# Calcualtes the document frequency of each word\n",
        "# First it convers the data into (word, doc_id) for easy processing.\n",
        "# Next we remove any duplicates.\n",
        "# We then convert to (word, 1) so we can sum the words.\n",
        "# Then finally, we merge words and count the number merged.\n",
        "df_rdd = (\n",
        "    tf_rdd  # (word, (doc_id, tf_value))\n",
        "    .map(lambda x: (x[0], x[1][0]))  # Extract (word, doc_id)\n",
        "    .distinct()  # Remove duplicate (word, doc_id) pairs\n",
        "    .map(lambda x: (x[0], 1))  # Convert to (word, 1)\n",
        "    .reduceByKey(lambda x, y: x + y)  # Sum counts per word\n",
        "    .sortBy(lambda x: x[0])\n",
        ")\n",
        "\n",
        "# print(df_rdd.take(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_FPPg9YS4VV"
      },
      "source": [
        "**From df_rdd, compute inverted document frequency (idf) of each word and put the results in another RDD idf_rdd whose element is (word, idf)**\n",
        "\n",
        "The idf value of a word is computed as $log\\frac{N}{df+1}$, where $N$ is the total number of documents and df is document frequency of the word. The \"1\" in denominator (df+1) prevents zero division.\n",
        "Hint: use df_rdd.mapValues()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "u_-h2tWcbv1s"
      },
      "outputs": [],
      "source": [
        "# Compute Inverted Document Frequency (IDF)\n",
        "\n",
        "# Your Code\n",
        "\n",
        "# Get the total number of documents.\n",
        "num_docs = text_rdd.count()\n",
        "\n",
        "# Calculate the inverted document frequency.\n",
        "# idf = log(number of documents / (document frequency + 1))\n",
        "idf_rdd = df_rdd.mapValues(lambda x: math.log(num_docs / (x + 1)))\n",
        "\n",
        "# print(idf_rdd.take(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeewFTmiUJX8"
      },
      "source": [
        "**From tf_rdd and idf_rdd, compute the tf-idf value of a word for each document and put the results in another RDD tfidf_rdd whose element is (word, (doc_id, tf-idf))**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "dzE503R-cY2E"
      },
      "outputs": [],
      "source": [
        "# Compute TF-IDF\n",
        "\n",
        "# Your Code\n",
        "\n",
        "# To calculate the tf-idf we first join the tf and idf RDDs to make calculations simpler.\n",
        "# We then sort for easier debugging and testing.\n",
        "# Format comes out as (word, ((doc_id, tf_value), idf_value)).\n",
        "tfidf_rdd = tf_rdd.join(idf_rdd).sortBy(lambda x: x[0])\n",
        "\n",
        "# print(tfidf_rdd.take(5))\n",
        "\n",
        "\n",
        "# Once merged we have access to each word, doc_id, and all the tf and idf values.\n",
        "# This now makes it easy to restructure the RDD.\n",
        "# We use map to rearrange the values, and caclulate the tf-idf value.\n",
        "# tf-idf = tf * idf\n",
        "tfidf_rdd = tfidf_rdd.map(lambda x: (x[0], (x[1][0][0], x[1][0][1] * x[1][1])))\n",
        "\n",
        "# print(tfidf_rdd.take(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH78_35Seo5A"
      },
      "source": [
        "**Convert tfidf_rdd to an inverted index RDD whose element is (word, [(doc_id, tf-idf), (doc_id, tf-idf), ...])**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "lp9oQHY7bxLu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf03759b-e04b-4cfb-dcb1-c94865c9b3e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('aacr', [('920', 0.12487346012441652), ('995', 0.17170100767107271), ('1000', 0.23380562746699263), ('1002', 0.2680210851450891), ('1003', 0.22426254063160517)]), ('abandon', [('612', 0.09364916434257922), ('848', 0.07283823893311718), ('1425', 0.046092948074863214)]), ('abbrevi', [('516', 0.07222618292330922), ('563', 0.17486339023538022), ('600', 0.048858888448120946), ('657', 0.08592425209841958), ('735', 0.048858888448120946), ('851', 0.1661202207236112), ('1326', 0.20765027590451401), ('97', 0.06644808828944448), ('442', 0.09967213243416673)]), ('aberrystwyth', [('817', 0.11367318162314546)]), ('abidjan', [('1239', 0.08241305667678046)])]\n"
          ]
        }
      ],
      "source": [
        "# Convert to Inverted Index Format\n",
        "\n",
        "# Your Code\n",
        "\n",
        "# Converts to inverted index format by grouping\n",
        "# keys to together, and making their values a series of tuples\n",
        "# formatted as (doc_id, tf-idf).\n",
        "# We also sort for easier testing and debugging.\n",
        "inverted_index_rdd = tfidf_rdd.groupByKey().mapValues(list).sortByKey()\n",
        "\n",
        "print(inverted_index_rdd.take(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "collapsed": true,
        "id": "0N0VVsELb8is",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31d670e7-b898-4bfc-d156-91f617f373d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aacr: [('920', 0.12487346012441652), ('995', 0.17170100767107271), ('1000', 0.23380562746699263), ('1002', 0.2680210851450891), ('1003', 0.22426254063160517)]\n",
            "abandon: [('612', 0.09364916434257922), ('848', 0.07283823893311718), ('1425', 0.046092948074863214)]\n",
            "abbrevi: [('516', 0.07222618292330922), ('563', 0.17486339023538022), ('600', 0.048858888448120946), ('657', 0.08592425209841958), ('735', 0.048858888448120946), ('851', 0.1661202207236112), ('1326', 0.20765027590451401), ('97', 0.06644808828944448), ('442', 0.09967213243416673)]\n",
            "aberrystwyth: [('817', 0.11367318162314546)]\n",
            "abidjan: [('1239', 0.08241305667678046)]\n"
          ]
        }
      ],
      "source": [
        "# Print a sample of the TF-IDF based inverted index\n",
        "for word, postings in inverted_index_rdd.take(5):\n",
        "    print(f\"{word}: {postings}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "R4I4UmqaYiXV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77db92bf-0ce8-4f1f-f99d-4b36d3568c4f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5545"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ],
      "source": [
        "# See how many words in the inverted index\n",
        "inverted_index_rdd.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKsPsi6y2vCA"
      },
      "source": [
        "# Part 3: Vector Space Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Query processing: define a function to convert a query into its tf-idf representation**"
      ],
      "metadata": {
        "id": "smd52xGeLtHc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "hdjjyvjh3Bfx"
      },
      "outputs": [],
      "source": [
        "# Put the word IDF values in a dictionary for fast lookup during query processing\n",
        "idf_lookup = idf_rdd.collectAsMap()\n",
        "\n",
        "# Function to process a query into a sparse TF-IDF dictionary\n",
        "def process_query(query):\n",
        "    \"\"\"\n",
        "    Process a query into a sparse TF-IDF dictionary.\n",
        "    Args:\n",
        "        query: The query string.\n",
        "    Returns:\n",
        "        A dictionary mapping words to their TF-IDF values.\n",
        "    \"\"\"\n",
        "\n",
        "    # York Code\n",
        "\n",
        "    # Apply tokenization provided by professor.\n",
        "    words = re.findall(r\"\\b[a-z]+\\b\", query.lower())\n",
        "\n",
        "    # Processes a query by filtering out stop words, and stemming.\n",
        "    # Initialize lists.\n",
        "    filtered_words = []\n",
        "    stemmed_words = []\n",
        "\n",
        "    # Loop through each word and only keep the ones that arent stop words.\n",
        "    for word in words:\n",
        "      if word not in stop_words:\n",
        "        filtered_words.append(word)\n",
        "\n",
        "    # Stem the filtered words.\n",
        "    for word in filtered_words:\n",
        "      stemmed_words.append(stemmer.stem(word))\n",
        "\n",
        "    # Count the frequency of each of the words, and store the values in a dictionary.\n",
        "    frequency_words = {}\n",
        "    for word in stemmed_words:\n",
        "      if word in frequency_words:\n",
        "        frequency_words[word] += 1\n",
        "      else:\n",
        "        frequency_words[word] = 1\n",
        "\n",
        "    # Get the total number of filtered and stemmed words.\n",
        "    len_words = len(stemmed_words)\n",
        "\n",
        "    # Get the percentage of each word to total number of words, and store the values in a dictionary.\n",
        "    frequency_words_percentage = {}\n",
        "    for word in frequency_words:\n",
        "      # percentage = frequency of words / total number of words\n",
        "      frequency_words_percentage[word] = frequency_words[word] / len_words\n",
        "\n",
        "\n",
        "    # Calculates the tfidf each word by multiplying by the idf value.\n",
        "    query_tfidf = {}\n",
        "    # Loop through each word, calculating the idf.\n",
        "    for word in frequency_words_percentage:\n",
        "      # Checks to make sure the idf exists for the word.\n",
        "      if(idf_lookup.get(word, 0)):\n",
        "        # percentage of the word * idf of word = tfidf\n",
        "        query_tfidf[word] = frequency_words_percentage[word] * idf_lookup.get(word)\n",
        "\n",
        "\n",
        "    return query_tfidf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3xSgJfN4UBB"
      },
      "source": [
        "**Precompute Document Norms:**\n",
        "The L2 norm of each document is computed once to speed up document score computation. This reduces redundant calculations at query time. This computation should be done based on the inverted index.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Precompute document norms based on their words' tf-idf values and put the norms in a Python dictionary with key being doc_id and value being the document norm\n",
        "\n",
        "# Your Code\n",
        "\n",
        "# Initialize dictionary to store document norms.\n",
        "doc_norms = {}\n",
        "\n",
        "# Loop through each word in the inverted index.\n",
        "for word in inverted_index_rdd.collect():\n",
        "\n",
        "  # We accumulate the squared tf-idf values.\n",
        "  # If the doc_id is not yet in doc_norms, then we insert the sqared tf-idf value,\n",
        "  # otherwise we add on the squared tf-idf value to the existing sum.\n",
        "  for doc in word[1]:\n",
        "    if doc[0] not in doc_norms:\n",
        "      doc_norms[doc[0]] = doc[1] ** 2\n",
        "    else:\n",
        "      doc_norms[doc[0]] = doc_norms[doc[0]] + (doc[1] ** 2)\n",
        "\n",
        "# Take the square root of each of the values to get the final\n",
        "# document norm value.\n",
        "for norm in doc_norms:\n",
        "  doc_norms[norm] = math.sqrt(doc_norms[norm])\n"
      ],
      "metadata": {
        "id": "inFZ9sno3pUJ"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define a function that computes document scores given a query and rank the documents according to their scores.**\n",
        "\n",
        "The score of a document **d** for query **q** is $\\frac{\\bf{q}\\cdot \\bf{d}}{\\bf{||d||}}$. Note that we ignore ||q|| in the denominator because it is the same  for all documents and thus does not affect document ranking. The computation of the dot product should use the sparse representation of the query and the inverted index as we discussed in class."
      ],
      "metadata": {
        "id": "mbTqarPOPBNF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "IYF7h4yTIN4Z"
      },
      "outputs": [],
      "source": [
        "# Function to compute document scores given a query using the vector space model\n",
        "def rank_documents(query, top_k):\n",
        "    \"\"\"\n",
        "    Rank documents based on a query using the vector space model.\n",
        "    Args:\n",
        "        query: The query string.\n",
        "        top_k: The number of top documents to return.\n",
        "    Returns:\n",
        "        A list of (doc_id, score) tuples, sorted by score in descending order.\n",
        "    \"\"\"\n",
        "\n",
        "    # Your Code\n",
        "\n",
        "    # Process the query and get the resulting words.\n",
        "    words = process_query(query)\n",
        "\n",
        "    # Converts the inverted index to a dictionary.\n",
        "    inverted_index = dict(inverted_index_rdd.collect())\n",
        "\n",
        "    # Initialize document scores dictionary.\n",
        "    doc_scores = {}\n",
        "\n",
        "    # Loop through each word in the query.\n",
        "    for word in words:\n",
        "\n",
        "      # Get all the documents containing that word.\n",
        "      tfidfs = inverted_index.get(word)\n",
        "\n",
        "      # For each document we calculate the product of the document tf-idf and\n",
        "      # the query tf-idf.\n",
        "      # We sum these values for each document so we can then divide by the\n",
        "      # doc norm afterwards.\n",
        "      for tfidf in tfidfs:\n",
        "\n",
        "        # If not yet in doc_scores, then we insert it into the dictionary,\n",
        "        # otherwise we sum the product onto the existing value.\n",
        "        if(tfidf[0] not in doc_scores):\n",
        "          doc_scores[tfidf[0]] = tfidf[1] * words[word]\n",
        "        else:\n",
        "          doc_scores[tfidf[0]] += tfidf[1] * words[word]\n",
        "\n",
        "    # Here we calculate the Cosine Similarity by dividing the sums from above\n",
        "    # by the document norm.\n",
        "    for score in doc_scores:\n",
        "      doc_scores[score] = doc_scores[score] / doc_norms[score]\n",
        "\n",
        "    # Initialize a list to append scores in tuple format (doc_id, score).\n",
        "    ranked_docs = []\n",
        "\n",
        "    # Add each score the the ranked list.\n",
        "    for score in doc_scores:\n",
        "      ranked_docs.append((score, doc_scores[score]))\n",
        "\n",
        "    # Sort the documents based on the document scores.\n",
        "    # Return the top_k number of documents.\n",
        "    ranked_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    ranked_docs = ranked_docs[:top_k]\n",
        "\n",
        "    return ranked_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Given a query, call the rank_documents function and print the top-10 documents**"
      ],
      "metadata": {
        "id": "kRKbT5hLml2V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "collapsed": true,
        "id": "kP0xzGSKnJdm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "051bee2c-8274-497b-fcfb-551f1e17bc47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'specif': 0.5048035559809252, 'advantag': 0.7194624521176892, 'computer': 0.6872088225984647, 'index': 0.3489856339087912, 'system': 0.34510201673737084}\n",
            "\n",
            "Top 10 Relevant Documents:\n",
            "\n",
            "Doc_ID: Document Score\n",
            "377: 0.3431657131018793\n",
            "869: 0.30415914371393965\n",
            "824: 0.23371892909299502\n",
            "1144: 0.2224614789422014\n",
            "856: 0.2212078128278848\n",
            "1419: 0.21683483722162045\n",
            "715: 0.20666101963028413\n",
            "812: 0.20652041350135483\n",
            "489: 0.19080310866870978\n",
            "194: 0.18652349192687345\n"
          ]
        }
      ],
      "source": [
        "# Example Query 1\n",
        "query = \"Specific advantages of computerized index systems.\"\n",
        "print(process_query(query))\n",
        "top_docs = rank_documents(query, 10)\n",
        "\n",
        "# Print top 10 relevant documents\n",
        "print(\"\\nTop 10 Relevant Documents:\")\n",
        "print(\"\\nDoc_ID: Document Score\")\n",
        "for doc, score in top_docs:\n",
        "    print(f\"{doc}: {score}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Query 2\n",
        "query = \"How can actually pertinent data, as opposed to references or entire articles themselves, be retrieved automatically in response to information requests?\"\n",
        "top_docs = rank_documents(query, 10)\n",
        "\n",
        "# Print top 10 relevant documents\n",
        "print(\"\\nTop 10 Relevant Documents:\")\n",
        "print(\"\\nDoc_ID: Document Score\")\n",
        "for doc, score in top_docs:\n",
        "    print(f\"{doc}: {score}\")"
      ],
      "metadata": {
        "id": "buL3WXHSsT5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3189404d-51d8-46e7-ecfa-16a2a4cbcf56"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 10 Relevant Documents:\n",
            "\n",
            "Doc_ID: Document Score\n",
            "1138: 0.2698882732022883\n",
            "532: 0.26532484401971396\n",
            "1155: 0.1958742695858231\n",
            "562: 0.15561758487265828\n",
            "309: 0.15488168234340965\n",
            "790: 0.1425356353084291\n",
            "1327: 0.14038899967739987\n",
            "451: 0.1385800761817236\n",
            "315: 0.1383328293087868\n",
            "58: 0.13745453463399604\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}